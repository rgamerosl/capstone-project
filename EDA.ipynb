{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import required libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import seaborn as sns\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make sure that 'ggplot' style is used for all plots\n",
    "plt.style.use('ggplot')\n",
    "# plt.style.available ### To view all other available styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set Working Directory (WD)\n",
    "os.chdir('/Volumes/GoogleDrive/My Drive/CEMEX/Data Translators/GitHub/rgamerosl/capstone-project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### How to import RDS (equivalent to RData) into pandas\n",
    "\n",
    "# import rpy2.robjects as robjects\n",
    "# from rpy2.robjects.packages import importr\n",
    "# from rpy2.robjects import pandas2ri\n",
    "\n",
    "# from rpy2.robjects.conversion import localconverter\n",
    "\n",
    "# readRDS = robjects.r['readRDS']\n",
    "# rdata = readRDS('dataset/Fuel_Data.RDS')\n",
    "\n",
    "# with localconverter(robjects.default_converter + pandas2ri.converter):\n",
    "#   pdata = robjects.conversion.rpy2py(rdata)\n",
    "\n",
    "# print(pdata.info())\n",
    "# display(pdata.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read the data\n",
    "df = pd.read_csv(\"dataset/Fuel_Data.csv\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fill with 0 the NA for the different events\n",
    "df.iloc[:,16:33] = df.iloc[:,16:33].fillna(0)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = df.drop(['Date','Plate','Zone','Hrs_eff','Engine_hrs','Fuel_used','km_per_liter'], axis=1)\n",
    "display(df0.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.subplots(figsize=(10,10))\n",
    "ax = sns.heatmap(df0.iloc[:,0:11].corr(), annot=True, fmt='0.2f', cmap='Blues')\n",
    "plt.yticks(rotation=0)\n",
    "plt.savefig(f'figures/correlations1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = [7,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25]\n",
    "fig = plt.subplots(figsize=(16,16))\n",
    "ax = sns.heatmap(df0.iloc[:,subset].corr(), annot=True, fmt='0.2f', cmap='Blues',xticklabels=subset,yticklabels=subset)\n",
    "plt.yticks(rotation=0)\n",
    "plt.savefig(f'figures/correlations2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df0.dropna(subset=['liters_per_hour'])\n",
    "df1.reset_index(inplace=True)\n",
    "df1.drop('index',axis=1,inplace=True)\n",
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1.to_excel('dataset/data_v2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oe_manufacturer = OneHotEncoder()\n",
    "oe_results_m = oe_manufacturer.fit_transform(df1[['Manufacturer']])\n",
    "manufacturer_ohe = pd.DataFrame(oe_results_m.toarray(), columns=oe_manufacturer.categories_)\n",
    "print(display(manufacturer_ohe.head(10)))\n",
    "manufacturer_ohe.columns=np.array(oe_manufacturer.categories_).flatten()\n",
    "manufacturer_ohe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.join(manufacturer_ohe)\n",
    "### Drop column for Kenworth, before droping it the 29 column corresponded to the manufacturer Kenworth\n",
    "df2.drop(df2.columns[29],axis=1,inplace=True)\n",
    "print(display(df2.head(10)))\n",
    "df2.info()"
   ]
  },
  {
   "source": [
    "### Weekdays (0: Monday to 6: Sunday)\n",
    "oe_weekday = OneHotEncoder()\n",
    "oe_results_w = oe_weekday.fit_transform(df2[['Weekday']])\n",
    "weekday_ohe = pd.DataFrame(oe_results_w.toarray(), columns=['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'])\n",
    "print(display(weekday_ohe.head(10)))\n",
    "weekday_ohe.columns=['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']\n",
    "weekday_ohe.info()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.join(weekday_ohe)\n",
    "### Drop column for Friday, before droping it the 34 column corresponded to the label Friday\n",
    "df2.drop(df2.columns[34],axis=1,inplace=True)\n",
    "print(display(df2.head(10)))\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oe_city = OneHotEncoder()\n",
    "oe_results_c = oe_city.fit_transform(df2[['City']])\n",
    "city_ohe = pd.DataFrame(oe_results_c.toarray(), columns=oe_city.categories_)\n",
    "print(display(city_ohe.head(10)))\n",
    "city_ohe.columns=np.array(oe_city.categories_).flatten()\n",
    "city_ohe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.join(city_ohe)\n",
    "### Drop column for MEXICO DF, before droping it the 80 column corresponded to the label MEXICO DF\n",
    "df2.drop(df2.columns[80],axis=1,inplace=True)\n",
    "print(display(df2.head(10)))\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Another approach to categorical/indicator variables using get_dummiyes properly\n",
    "# import pandas as pd\n",
    "\n",
    "# from pandas.api.types import CategoricalDtype \n",
    "\n",
    "# # say you want a column for \"japan\" too (it'll be always zero, of course)\n",
    "# df[\"country\"] = train_df[\"country\"].astype(CategoricalDtype([\"australia\",\"germany\",\"korea\",\"russia\",\"japan\"]))\n",
    "\n",
    "# # now call .get_dummies() as usual\n",
    "# pd.get_dummies(df[\"country\"],prefix='country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.to_excel(\"dataset/final_data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df2.drop(['Manufacturer','City','Weekday'],axis=1)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now need to do Train Test split and afterwards StandardScale all the numerical variables in each set seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train Test split\n",
    "data_train, data_test = train_test_split(data, test_size=0.25, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_indexes = data.columns[0:23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Standarize numerical variables in Train Set\n",
    "scaler = StandardScaler()\n",
    "data_train_scale = data_train.copy(deep=True)\n",
    "data_train_scale[col_indexes] = scaler.fit_transform(data_train[col_indexes].to_numpy()) \n",
    "display(data_train_scale.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Standarize numerical variables in Test Set\n",
    "scaler = StandardScaler()\n",
    "data_test_scale = data_test.copy(deep=True)\n",
    "data_test_scale[col_indexes] = scaler.fit_transform(data_test[col_indexes].to_numpy()) \n",
    "display(data_test_scale.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}