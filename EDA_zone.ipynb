{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import required libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import seaborn as sns\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make sure that 'ggplot' style is used for all plots\n",
    "plt.style.use('ggplot')\n",
    "# plt.style.available ### To view all other available styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set Working Directory (WD)\n",
    "os.chdir('/Volumes/GoogleDrive/My Drive/CEMEX/Data Translators/GitHub/rgamerosl/capstone-project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### How to import RDS (equivalent to RData) into pandas\n",
    "\n",
    "# import rpy2.robjects as robjects\n",
    "# from rpy2.robjects.packages import importr\n",
    "# from rpy2.robjects import pandas2ri\n",
    "\n",
    "# from rpy2.robjects.conversion import localconverter\n",
    "\n",
    "# readRDS = robjects.r['readRDS']\n",
    "# rdata = readRDS('dataset/Fuel_Data.RDS')\n",
    "\n",
    "# with localconverter(robjects.default_converter + pandas2ri.converter):\n",
    "#   pdata = robjects.conversion.rpy2py(rdata)\n",
    "\n",
    "# print(pdata.info())\n",
    "# display(pdata.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read the data\n",
    "df = pd.read_csv(\"dataset/Fuel_Data.csv\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fill with 0 the NA for the different events\n",
    "df.iloc[:,16:33] = df.iloc[:,16:33].fillna(0)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = df.drop(['Date','Plate','City','Hrs_eff','Engine_hrs','Fuel_used','km_per_liter'], axis=1)\n",
    "display(df0.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.subplots(figsize=(10,10))\n",
    "ax = sns.heatmap(df0.iloc[:,0:11].corr(), annot=True, fmt='0.2f', cmap='Blues')\n",
    "plt.yticks(rotation=0)\n",
    "# plt.savefig(f'figures/correlations1.png')\n",
    "plt.show()\n",
    "\n",
    "### Since Speed and mileage seems to be closely related, probably I should only keep one of them. Sam thing for Trips and Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = [7,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25]\n",
    "fig = plt.subplots(figsize=(16,16))\n",
    "ax = sns.heatmap(df0.iloc[:,subset].corr(), annot=True, fmt='0.2f', cmap='Blues',xticklabels=subset,yticklabels=subset)\n",
    "plt.yticks(rotation=0)\n",
    "# plt.savefig(f'figures/correlations2.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df0.dropna(subset=['liters_per_hour'])\n",
    "df1.reset_index(inplace=True)\n",
    "df1.drop('index',axis=1,inplace=True)\n",
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1.to_excel('dataset/data_v2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oe_manufacturer = OneHotEncoder()\n",
    "oe_results_m = oe_manufacturer.fit_transform(df1[['Manufacturer']])\n",
    "manufacturer_ohe = pd.DataFrame(oe_results_m.toarray(), columns=oe_manufacturer.categories_)\n",
    "print(display(manufacturer_ohe.head(10)))\n",
    "manufacturer_ohe.columns=np.array(oe_manufacturer.categories_).flatten()\n",
    "manufacturer_ohe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.join(manufacturer_ohe)\n",
    "### Drop column for Kenworth, before droping it the 29 column corresponded to the manufacturer Kenworth\n",
    "df2.drop(df2.columns[29],axis=1,inplace=True)\n",
    "print(display(df2.head(10)))\n",
    "df2.info()"
   ]
  },
  {
   "source": [
    "### Weekdays (0: Monday to 6: Sunday)\n",
    "oe_weekday = OneHotEncoder()\n",
    "oe_results_w = oe_weekday.fit_transform(df2[['Weekday']])\n",
    "weekday_ohe = pd.DataFrame(oe_results_w.toarray(), columns=['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'])\n",
    "print(display(weekday_ohe.head(10)))\n",
    "weekday_ohe.columns=['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']\n",
    "weekday_ohe.info()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.join(weekday_ohe)\n",
    "### Drop column for Friday, before droping it the 34 column corresponded to the label Friday\n",
    "df2.drop(df2.columns[34],axis=1,inplace=True)\n",
    "print(display(df2.head(10)))\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oe_zone = OneHotEncoder()\n",
    "oe_results_z = oe_zone.fit_transform(df2[['Zone']])\n",
    "zone_ohe = pd.DataFrame(oe_results_z.toarray(), columns=oe_zone.categories_)\n",
    "print(display(zone_ohe.head(10)))\n",
    "zone_ohe.columns=np.array(oe_zone.categories_).flatten()\n",
    "zone_ohe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.join(zone_ohe)\n",
    "### Drop column for PAV, before droping it the 39 column corresponded to the label PAV (Pavimentos)\n",
    "df2.drop(df2.columns[39],axis=1,inplace=True)\n",
    "print(display(df2.head(10)))\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Another approach to categorical/indicator variables using get_dummiyes properly\n",
    "# import pandas as pd\n",
    "\n",
    "# from pandas.api.types import CategoricalDtype \n",
    "\n",
    "# # say you want a column for \"japan\" too (it'll be always zero, of course)\n",
    "# df[\"country\"] = train_df[\"country\"].astype(CategoricalDtype([\"australia\",\"germany\",\"korea\",\"russia\",\"japan\"]))\n",
    "\n",
    "# # now call .get_dummies() as usual\n",
    "# pd.get_dummies(df[\"country\"],prefix='country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.to_excel(\"dataset/final_data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df2.drop(['Manufacturer','Zone','Weekday'],axis=1)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=['Mileage'],axis=0)\n",
    "data.reset_index(inplace=True)\n",
    "data.drop('index',axis=1,inplace=True)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now need to do Train Test split and afterwards StandardScale all the numerical variables in each set seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train Test split\n",
    "data_train, data_test = train_test_split(data, test_size=0.25, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_indexes = data.columns[0:23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Standarize numerical variables in Train Set\n",
    "scaler = StandardScaler()\n",
    "data_train_scale = data_train.copy(deep=True)\n",
    "data_train_scale[col_indexes] = scaler.fit_transform(data_train[col_indexes].to_numpy()) \n",
    "display(data_train_scale.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Standarize numerical variables in Test Set\n",
    "scaler = StandardScaler()\n",
    "data_test_scale = data_test.copy(deep=True)\n",
    "data_test_scale[col_indexes] = scaler.fit_transform(data_test[col_indexes].to_numpy()) \n",
    "display(data_test_scale.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_train_scale.loc[:, data_train_scale.columns != 'liters_per_hour'].values\n",
    "y_train = data_train_scale['liters_per_hour'].values\n",
    "\n",
    "X_test = data_test_scale.loc[:, data_test_scale.columns != 'liters_per_hour'].values\n",
    "y_test = data_test_scale['liters_per_hour'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cval_model(X_train,y_train, model='RandomForest', cv=True, LR=0.1):\n",
    "    '''\n",
    "    runs crossvalidation in the specified model and returns the MSE and R2 metrics according to the train dataset\n",
    "    '''\n",
    "    scores = np.zeros(2)\n",
    "    if model == 'RandomForest':\n",
    "        model = RandomForestRegressor(n_estimators=100, n_jobs=-1, random_state=1)\n",
    "    elif model == 'GradientBoostin':\n",
    "        model = GradientBoostingRegressor(learning_rate=LR, n_estimators=100, random_state=1)\n",
    "    else:\n",
    "        model = AdaBoostRegressor(DecisionTreeClassifier(), learning_rate=LR, n_estimators=100, random_state=1)\n",
    "    model.fit(X_train, y_train)\n",
    "    if cv:\n",
    "        scores[0] = -1*cross_val_score(model, X_train, y_train, scoring= 'neg_mean_squared_error', cv=5).mean()\n",
    "        scores[1] = cross_val_score(model, X_train, y_train, scoring= 'r2', cv=5).mean()\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_rf = cval_model(X_train,y_train,model='RandomForest',cv=True)\n",
    "scores_gdbr = cval_model(X_train,y_train,model='GradientBoostin',cv=True)\n",
    "\n",
    "print(f'RandomForestRegressor       Train CV    |   MSE: {round(scores_rf[0],3)}   |   R2: {round(scores_rf[1],3)}')\n",
    "print(f'GradientBoostinRegressor    Train CV    |   MSE: {round(scores_gdbr[0],3)}  |   R2: {round(scores_gdbr[1],3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Here I will do a GridSearch to tune the parameters used in the RandomForestRegressor model\n",
    "random_forest_grid = {'max_depth': [3, None],\n",
    "                      'max_features': ['sqrt', 'log2', None],\n",
    "                      'min_samples_split': [2, 4],\n",
    "                      'min_samples_leaf': [1, 2, 4],\n",
    "                      'bootstrap': [True, False],\n",
    "                      'n_estimators': [10, 20, 40, 80],\n",
    "                      'random_state': [1]}\n",
    "\n",
    "rf_gridsearch = GridSearchCV(RandomForestRegressor(),\n",
    "                             random_forest_grid,\n",
    "                             n_jobs=-1,\n",
    "                             verbose=True,\n",
    "                             scoring='neg_mean_squared_error')\n",
    "\n",
    "rf_gridsearch.fit(X_train, y_train)\n",
    "\n",
    "print(\"best parameters:\", rf_gridsearch.best_params_)\n",
    "\n",
    "best_rf_model = rf_gridsearch.best_estimator_\n",
    "\n",
    "# Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n",
    "# [Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
    "# [Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   42.4s\n",
    "# [Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  4.9min\n",
    "# [Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed: 27.7min\n",
    "# [Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed: 80.4min\n",
    "# [Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed: 124.7min\n",
    "# [Parallel(n_jobs=-1)]: Done 1440 out of 1440 | elapsed: 186.5min finished\n",
    "# best parameters: {'bootstrap': False, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 80, 'random_state': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adjusting Best model to answer the following questions\n",
    "\n",
    "best_rf = RandomForestRegressor(n_estimators=80, n_jobs=-1, random_state=1, max_features='sqrt',\n",
    "                                min_samples_leaf=2, min_samples_split=2, max_depth=None, bootstrap=False)\n",
    "best_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_y_train_pred = best_rf.predict(X_train)\n",
    "best_rf_train_MSE_score = mean_squared_error(y_train, best_rf_y_train_pred)\n",
    "print(\"MSE for the Best Random Forest in the Train data:\", round(best_rf_train_MSE_score,4))\n",
    "best_rf_train_R2_score = r2_score(y_train, best_rf_y_train_pred)\n",
    "print(\"R2 for the Best Random Forest in the Train data:\", round(best_rf_train_R2_score,4))\n",
    "\n",
    "best_rf_y_test_pred = best_rf.predict(X_test)\n",
    "best_rf_test_MSE_score = mean_squared_error(y_test, best_rf_y_test_pred)\n",
    "print(\"MSE for the Best Random Forest in the Test data:\", round(best_rf_test_MSE_score,4))\n",
    "best_rf_test_R2_score = r2_score(y_test, best_rf_y_test_pred)\n",
    "print(\"R2 for the Best Random Forest in the Test data:\", round(best_rf_test_R2_score,4))\n",
    "\n",
    "# MSE for the Best Random Forest in the Train data: 0.0352\n",
    "# R2 for the Best Random Forest in the Train data: 0.9648\n",
    "# MSE for the Best Random Forest in the Test data: 0.2412\n",
    "# R2 for the Best Random Forest in the Test data: 0.7588\n",
    "\n",
    "### Due to the variation between train and test datasets I think there could be some OVERFITTING going on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = best_rf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in best_rf.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.barh(range(X_train.shape[1]), importances[indices],\n",
    "       color=\"r\", xerr=std[indices], align=\"center\")\n",
    "# If you want to define your own labels,\n",
    "# change indices to a list of labels on the following line.\n",
    "plt.yticks(range(X_train.shape[1]), data_train_scale.loc[:, data_train_scale.columns != 'liters_per_hour'].columns[indices[::1]])\n",
    "plt.ylim([-1, X_train.shape[1]])\n",
    "plt.savefig(f'figures/feature_importances.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_scale.loc[:, data_train_scale.columns != 'liters_per_hour'].columns[indices[::1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_columns = data_train_scale.loc[:, data_train_scale.columns != 'liters_per_hour'].columns[indices[::-1]][0:21].values.tolist()\n",
    "relevant_columns.append('liters_per_hour')\n",
    "new_data = data[relevant_columns]\n",
    "new_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.drop(['Mileage','Volume'],axis=1,inplace=True)\n",
    "new_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train Test split\n",
    "new_data_train, new_data_test = train_test_split(new_data, test_size=0.25, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = new_data.columns[[0,1,2,3,4,9,10,11,13,14,15,16,17,18,19]]\n",
    "print(numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Standarize numerical variables in Train Set\n",
    "scaler = StandardScaler()\n",
    "new_data_train_scale = new_data_train.copy(deep=True)\n",
    "new_data_train_scale[numeric_cols] = scaler.fit_transform(new_data_train[numeric_cols].to_numpy()) \n",
    "display(new_data_train_scale.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Standarize numerical variables in Test Set\n",
    "scaler = StandardScaler()\n",
    "new_data_test_scale = new_data_test.copy(deep=True)\n",
    "new_data_test_scale[numeric_cols] = scaler.fit_transform(new_data_test[numeric_cols].to_numpy()) \n",
    "display(new_data_test_scale.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_train = new_data_train_scale.loc[:, new_data_train_scale.columns != 'liters_per_hour'].values\n",
    "new_y_train = new_data_train_scale['liters_per_hour'].values\n",
    "\n",
    "new_X_test = new_data_test_scale.loc[:, new_data_test_scale.columns != 'liters_per_hour'].values\n",
    "new_y_test = new_data_test_scale['liters_per_hour'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_scores_rf = cval_model(new_X_train,new_y_train,model='RandomForest',cv=True)\n",
    "new_scores_gdbr = cval_model(new_X_train,new_y_train,model='GradientBoostin',cv=True)\n",
    "\n",
    "print(f'RandomForestRegressor       Train CV    |   MSE: {round(new_scores_rf[0],3)}   |   R2: {round(new_scores_rf[1],3)}')\n",
    "print(f'GradientBoostinRegressor    Train CV    |   MSE: {round(new_scores_gdbr[0],3)}  |   R2: {round(new_scores_gdbr[1],3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adjusting Best model to answer the following questions\n",
    "\n",
    "new_best_rf = RandomForestRegressor(n_estimators=80, n_jobs=-1, random_state=1, max_features='sqrt',\n",
    "                                min_samples_leaf=2, min_samples_split=2, max_depth=None, bootstrap=False)\n",
    "new_best_rf.fit(new_X_train, new_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_best_rf_y_train_pred = new_best_rf.predict(new_X_train)\n",
    "new_best_rf_train_MSE_score = mean_squared_error(new_y_train, new_best_rf_y_train_pred)\n",
    "print(\"MSE for the Best Random Forest in the Train data:\", round(new_best_rf_train_MSE_score,4))\n",
    "new_best_rf_train_R2_score = r2_score(new_y_train, new_best_rf_y_train_pred)\n",
    "print(\"R2 for the Best Random Forest in the Train data:\", round(best_rf_train_R2_score,4))\n",
    "\n",
    "new_best_rf_y_test_pred = new_best_rf.predict(new_X_test)\n",
    "new_best_rf_test_MSE_score = mean_squared_error(new_y_test, new_best_rf_y_test_pred)\n",
    "print(\"MSE for the Best Random Forest in the Test data:\", round(new_best_rf_test_MSE_score,4))\n",
    "new_best_rf_test_R2_score = r2_score(new_y_test, new_best_rf_y_test_pred)\n",
    "print(\"R2 for the Best Random Forest in the Test data:\", round(new_best_rf_test_R2_score,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_importances = new_best_rf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in new_best_rf.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(new_importances)\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.barh(range(new_X_train.shape[1]), new_importances[indices],\n",
    "       color=\"r\", xerr=std[indices], align=\"center\")\n",
    "# If you want to define your own labels,\n",
    "# change indices to a list of labels on the following line.\n",
    "plt.yticks(range(new_X_train.shape[1]), new_data_train_scale.loc[:, new_data_train_scale.columns != 'liters_per_hour'].columns[indices[::1]])\n",
    "plt.ylim([-1, new_X_train.shape[1]])\n",
    "plt.savefig(f'figures/new_feature_importances.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Packages require to adjust Multiple Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train = sm.add_constant(X_train)\n",
    "est = sm.OLS(y_train, X2_train)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())\n",
    "\n",
    "### I did not expect all features to be relevant according to the p-value (except for 2 or 4 features only: 29, 30, 33, 35)\n",
    "### Doubt: How to interpret coefficients with non-standarized data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_lr = est2.predict(X2_train)\n",
    "lr_train_MSE_score = mean_squared_error(y_train, y_train_pred_lr)\n",
    "print(\"MSE for the Multiple Linear Regression in the Train data:\", round(lr_train_MSE_score,4))\n",
    "lr_train_R2_score = r2_score(y_train, y_train_pred_lr)\n",
    "print(\"R2 for the Multiple Linear Regression in the Train data:\", round(lr_train_R2_score,4))\n",
    "\n",
    "X2_test = sm.add_constant(X_test)\n",
    "y_test_pred_lr = est2.predict(X2_test)\n",
    "lr_test_MSE_score = mean_squared_error(y_test, y_test_pred_lr)\n",
    "print(\"MSE for the Multiple Linear Regression in the Test data:\", round(lr_test_MSE_score,4))\n",
    "lr_test_R2_score = r2_score(y_test, y_test_pred_lr)\n",
    "print(\"R2 for the Multiple Linear Regression in the Test data:\", round(lr_test_R2_score,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X2_train = sm.add_constant(new_X_train)\n",
    "new_est = sm.OLS(new_y_train, new_X2_train)\n",
    "new_est2 = new_est.fit()\n",
    "print(new_est2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_y_train_pred_lr = new_est2.predict(new_X2_train)\n",
    "new_lr_train_MSE_score = mean_squared_error(new_y_train, new_y_train_pred_lr)\n",
    "print(\"MSE for the Multiple Linear Regression in the Train data:\", round(new_lr_train_MSE_score,4))\n",
    "new_lr_train_R2_score = r2_score(new_y_train, new_y_train_pred_lr)\n",
    "print(\"R2 for the Multiple Linear Regression in the Train data:\", round(new_lr_train_R2_score,4))\n",
    "\n",
    "new_X2_test = sm.add_constant(new_X_test)\n",
    "new_y_test_pred_lr = new_est2.predict(new_X2_test)\n",
    "new_lr_test_MSE_score = mean_squared_error(new_y_test, new_y_test_pred_lr)\n",
    "print(\"MSE for the Multiple Linear Regression in the Test data:\", round(new_lr_test_MSE_score,4))\n",
    "new_lr_test_R2_score = r2_score(new_y_test, new_y_test_pred_lr)\n",
    "print(\"R2 for the Multiple Linear Regression in the Test data:\", round(new_lr_test_R2_score,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports require to do the VIF analysis\n",
    "from patsy import dmatrices\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_df = data.iloc[:,0:23]\n",
    "vif_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### First attempt to make this work, but I get some error... still need to look into it with more details\n",
    "features = \"+\".join(vif_df.columns - [\"liters_per_hour\"])\n",
    "vif_y, vif_X = dmatrices('liters_per_hour ~' + features, df, return_type='dataframe')\n",
    "\n",
    "vif_model = pd.DataFrame()\n",
    "vif_model[\"VIF Factor\"] = [variance_inflation_factor(vif_X.values, i) for i in range(vif_X.shape[1])]\n",
    "vif_model[\"features\"] = vif_X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}